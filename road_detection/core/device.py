#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¾å¤‡ç®¡ç†æ¨¡å—
Device management module
"""

import torch
import platform
from typing import Optional

def setup_device(prefer_gpu: bool = True) -> str:
    """
    é…ç½®è®­ç»ƒè®¾å¤‡
    
    Args:
        prefer_gpu: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨GPU
        
    Returns:
        è®¾å¤‡åç§° ('cuda' æˆ– 'cpu')
    """
    if prefer_gpu and torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        cuda_version = torch.version.cuda
        print(f"ğŸš€ GPUå¯ç”¨: {device_name}")
        print(f"ğŸ”§ CUDAç‰ˆæœ¬: {cuda_version}")
        
        # æ£€æŸ¥GPUå†…å­˜
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f} GB")
        
        # è®¾ç½®GPUä¼˜åŒ–
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        return 'cuda'
    else:
        print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ")
        cpu_count = torch.get_num_threads()
        print(f"ğŸ”§ CPUçº¿ç¨‹æ•°: {cpu_count}")
        
        # è®¾ç½®CPUä¼˜åŒ–
        if platform.system() == "Windows":
            torch.set_num_threads(min(cpu_count, 8))  # Windowsä¸‹é™åˆ¶çº¿ç¨‹æ•°
        else:
            torch.set_num_threads(cpu_count)
        
        return 'cpu'

def get_device_info() -> dict:
    """
    è·å–è®¾å¤‡ä¿¡æ¯
    
    Returns:
        è®¾å¤‡ä¿¡æ¯å­—å…¸
    """
    info = {
        'platform': platform.system(),
        'python_version': platform.python_version(),
        'torch_version': torch.__version__
    }
    
    if torch.cuda.is_available():
        info.update({
            'device': 'cuda',
            'device_name': torch.cuda.get_device_name(0),
            'cuda_version': torch.version.cuda,
            'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3,
            'gpu_count': torch.cuda.device_count()
        })
    else:
        info.update({
            'device': 'cpu',
            'cpu_count': torch.get_num_threads()
        })
    
    return info

def check_memory_requirements(batch_size: int, img_size: int, model_size: str = 'n') -> bool:
    """
    æ£€æŸ¥å†…å­˜éœ€æ±‚
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        img_size: å›¾åƒå°ºå¯¸
        model_size: æ¨¡å‹å¤§å° (n, s, m, l, x)
        
    Returns:
        æ˜¯å¦æ»¡è¶³å†…å­˜è¦æ±‚
    """
    if not torch.cuda.is_available():
        return True  # CPUè®­ç»ƒä¸æ£€æŸ¥å†…å­˜
    
    # ä¼°ç®—GPUå†…å­˜éœ€æ±‚ (GB)
    base_memory = {
        'n': 2.0,  # YOLOv11-n
        's': 3.5,  # YOLOv11-s
        'm': 5.0,  # YOLOv11-m
        'l': 8.0,  # YOLOv11-l
        'x': 12.0  # YOLOv11-x
    }
    
    # è®¡ç®—å†…å­˜éœ€æ±‚
    model_base = base_memory.get(model_size, 2.0)
    img_memory = (img_size ** 2) / (640 ** 2)  # ç›¸å¯¹640x640çš„å€æ•°
    batch_memory = batch_size / 16  # ç›¸å¯¹æ‰¹æ¬¡16çš„å€æ•°
    
    required_memory = model_base * img_memory * batch_memory
    
    # è·å–å¯ç”¨GPUå†…å­˜
    available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    # é¢„ç•™20%çš„å®‰å…¨è¾¹é™…
    safe_memory = available_memory * 0.8
    
    if required_memory > safe_memory:
        print(f"âš ï¸  å†…å­˜éœ€æ±‚è­¦å‘Š:")
        print(f"   éœ€è¦å†…å­˜: {required_memory:.1f} GB")
        print(f"   å¯ç”¨å†…å­˜: {available_memory:.1f} GB")
        print(f"   å»ºè®®é™ä½æ‰¹æ¬¡å¤§å°æˆ–å›¾åƒå°ºå¯¸")
        return False
    
    return True

def clear_gpu_cache():
    """æ¸…ç†GPUç¼“å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print("ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")

def set_random_seed(seed: int = 42):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    
    Args:
        seed: éšæœºç§å­
    """
    import random
    import numpy as np
    
    # Pythonéšæœºç§å­
    random.seed(seed)
    
    # Numpyéšæœºç§å­
    np.random.seed(seed)
    
    # PyTorchéšæœºç§å­
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿ç¡®å®šæ€§è¡Œä¸º
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")