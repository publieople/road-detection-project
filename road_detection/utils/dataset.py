#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†åˆ†æå·¥å…·æ¨¡å—
Dataset analysis utility module
"""

import yaml
from pathlib import Path
from collections import Counter
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import numpy as np

def get_dataset_stats(data_yaml_path: str) -> Dict[str, any]:
    """
    ä»æ•°æ®é…ç½®æ–‡ä»¶ä¸­è·å–ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        data_yaml_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    try:
        print(f"ğŸ“Š æ­£åœ¨åˆ†ææ•°æ®é›†é…ç½®: {data_yaml_path}")
        
        with open(data_yaml_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)
        
        # è·å–ç±»åˆ«ä¿¡æ¯
        nc = data_config.get('nc', 0)
        names = data_config.get('names', [])
        
        print(f"ğŸ¯ ç±»åˆ«æ•°é‡: {nc}")
        print(f"ğŸ·ï¸  ç±»åˆ«åç§°: {names}")
        
        # è·å–åŸºç¡€è·¯å¾„
        yaml_dir = Path(data_yaml_path).parent
        base_path = yaml_dir
        
        print(f"ğŸ“‚ YAMLæ–‡ä»¶æ‰€åœ¨ç›®å½•: {base_path}")
        
        # è®¡ç®—è®­ç»ƒå’ŒéªŒè¯å›¾ç‰‡æ•°é‡
        def count_images_and_labels(train_val_path):
            """è®¡ç®—æŒ‡å®šè·¯å¾„ä¸‹çš„å›¾ç‰‡å’Œæ ‡ç­¾æ•°é‡"""
            if not train_val_path:
                return 0, 0
            
            # æ„å»ºå®Œæ•´çš„å›¾ç‰‡è·¯å¾„
            image_path = base_path / train_val_path
            
            print(f"\nğŸ” æ£€æŸ¥è·¯å¾„: {image_path}")
            
            if not image_path.exists():
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {image_path}")
                return 0, 0
            
            # ç»Ÿè®¡å›¾ç‰‡æ–‡ä»¶
            image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']
            image_files = []
            
            # è·å–æ‰€æœ‰æ–‡ä»¶ï¼Œç„¶åæŒ‰æ‰©å±•åè¿‡æ»¤
            for file_path in image_path.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    image_files.append(file_path)
            
            # å»é‡ï¼ˆæŒ‰æ–‡ä»¶åï¼‰
            unique_files = list(set(image_files))
            total_images = len(unique_files)
            print(f"ğŸ“¸ æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {total_images} å¼ ")
            
            # æ£€æŸ¥å¯¹åº”çš„æ ‡ç­¾è·¯å¾„
            label_path = Path(str(image_path).replace('images', 'labels'))
            print(f"ğŸ·ï¸  æ ‡ç­¾è·¯å¾„: {label_path}")
            
            if label_path.exists():
                # ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶
                label_files = list(label_path.rglob('*.txt'))
                total_labels = len(label_files)
                print(f"ğŸ“ æ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {total_labels} ä¸ª")
                
                # æ£€æŸ¥åŒ¹é…æƒ…å†µ
                if total_images > 0:
                    match_ratio = (total_labels / total_images) * 100
                    print(f"âœ… å›¾ç‰‡-æ ‡ç­¾åŒ¹é…ç‡: {match_ratio:.1f}%")
                    
                    if match_ratio < 100:
                        print(f"âš ï¸  è­¦å‘Š: {total_images - total_labels} å¼ å›¾ç‰‡ç¼ºå°‘æ ‡ç­¾æ–‡ä»¶")
                        
                        # åˆ—å‡ºå‰10ä¸ªæ²¡æœ‰æ ‡ç­¾çš„å›¾ç‰‡
                        missing_labels = []
                        for img_file in image_files[:10]:  # åªæ£€æŸ¥å‰10ä¸ª
                            expected_label = label_path / (img_file.stem + '.txt')
                            if not expected_label.exists():
                                missing_labels.append(img_file.name)
                        
                        if missing_labels:
                            print(f"   ç¼ºå¤±æ ‡ç­¾çš„å›¾ç‰‡ç¤ºä¾‹: {missing_labels[:5]}")
            else:
                print(f"âŒ æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {label_path}")
                total_labels = 0
            
            return total_images, total_labels
        
        # ç»Ÿè®¡è®­ç»ƒé›†
        train_path = data_config.get('train', 'images/train')
        train_images, train_labels = count_images_and_labels(train_path)
        
        # ç»Ÿè®¡éªŒè¯é›†
        val_path = data_config.get('val', 'images/val')
        val_images, val_labels = count_images_and_labels(val_path)
        
        # æ€»è®¡
        total_images = train_images + val_images
        total_labels = train_labels + val_labels
        
        print(f"\næ•°æ®é›†ç»Ÿè®¡æ€»ç»“:")
        print("=" * 60)
        print(f"è®­ç»ƒé›†: {train_images} å¼ å›¾ç‰‡, {train_labels} ä¸ªæ ‡ç­¾")
        print(f"éªŒè¯é›†: {val_images} å¼ å›¾ç‰‡, {val_labels} ä¸ªæ ‡ç­¾")
        print(f"æ€»è®¡: {total_images} å¼ å›¾ç‰‡, {total_labels} ä¸ªæ ‡ç­¾")
        
        # YOLOè®­ç»ƒæ—¶çš„å®é™…ä½¿ç”¨æ•°é‡ï¼ˆæœ‰æ ‡ç­¾çš„å›¾ç‰‡ï¼‰
        usable_train = min(train_images, train_labels)
        usable_val = min(val_images, val_labels)
        usable_total = usable_train + usable_val
        
        print(f"\nYOLOè®­ç»ƒå®é™…å¯ç”¨:")
        print(f"   è®­ç»ƒé›†: {usable_train} å¼ å›¾ç‰‡")
        print(f"   éªŒè¯é›†: {usable_val} å¼ å›¾ç‰‡")
        print(f"   æ€»è®¡: {usable_total} å¼ å›¾ç‰‡")
        
        if usable_total < total_images:
            print(f"è­¦å‘Š: ç”±äºç¼ºå°‘æ ‡ç­¾æ–‡ä»¶ï¼ŒYOLOå°†åªä½¿ç”¨ {usable_total}/{total_images} å¼ å›¾ç‰‡")
        
        return {
            'train_count': train_labels,  # å®é™…æœ‰æ ‡ç­¾çš„è®­ç»ƒå›¾ç‰‡æ•°é‡
            'val_count': val_labels,      # å®é™…æœ‰æ ‡ç­¾çš„éªŒè¯å›¾ç‰‡æ•°é‡
            'total_images': total_images, # æ€»å›¾ç‰‡æ•°é‡
            'total_labels': total_labels, # æ€»æ ‡ç­¾æ•°é‡
            'num_classes': nc,
            'class_names': names
        }
        
    except Exception as e:
        print(f"âš ï¸  è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'train_count': 0,
            'val_count': 0,
            'total_images': 0,
            'total_labels': 0,
            'num_classes': 0,
            'class_names': []
        }

class DatasetAnalyzer:
    """æ•°æ®é›†åˆ†æå™¨ç±»"""
    
    def __init__(self, data_yaml_path: str):
        """
        åˆå§‹åŒ–æ•°æ®é›†åˆ†æå™¨
        
        Args:
            data_yaml_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.data_yaml_path = data_yaml_path
        self.data_config = None
        self.base_path = None
        self.class_distribution = None
        
        self._load_config()
    
    def _load_config(self):
        """åŠ è½½æ•°æ®é…ç½®"""
        try:
            with open(self.data_yaml_path, 'r', encoding='utf-8') as f:
                self.data_config = yaml.safe_load(f)
            
            self.base_path = Path(self.data_yaml_path).parent
            print(f"âœ… æ•°æ®é…ç½®åŠ è½½æˆåŠŸ: {self.data_yaml_path}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®é…ç½®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def analyze_class_distribution(self) -> Dict[int, int]:
        """
        åˆ†æç±»åˆ«åˆ†å¸ƒ
        
        Returns:
            ç±»åˆ«åˆ†å¸ƒå­—å…¸ {class_id: count}
        """
        print("ğŸ“Š åˆ†æç±»åˆ«åˆ†å¸ƒ...")
        
        class_counts = Counter()
        total_instances = 0
        
        # è·å–ç±»åˆ«æ•°é‡
        num_classes = self.data_config.get('nc', 0)
        
        # åˆ†æè®­ç»ƒé›†
        train_label_dir = self.base_path / 'labels' / 'train'
        if train_label_dir.exists():
            for txt_file in train_label_dir.glob("*.txt"):
                with open(txt_file, 'r') as f:
                    for line in f:
                        if line.strip():
                            class_id = int(line.strip().split()[0])
                            if class_id < num_classes:
                                class_counts[class_id] += 1
                                total_instances += 1
        
        # åˆ†æéªŒè¯é›†
        val_label_dir = self.base_path / 'labels' / 'val'
        if val_label_dir.exists():
            for txt_file in val_label_dir.glob("*.txt"):
                with open(txt_file, 'r') as f:
                    for line in f:
                        if line.strip():
                            class_id = int(line.strip().split()[0])
                            if class_id < num_classes:
                                class_counts[class_id] += 1
                                total_instances += 1
        
        # æ‰“å°åˆ†å¸ƒç»Ÿè®¡
        class_names = self.data_config.get('names', [])
        print("ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
        for class_id in range(num_classes):
            count = class_counts.get(class_id, 0)
            percentage = (count / total_instances * 100) if total_instances > 0 else 0
            class_name = class_names[class_id] if class_id < len(class_names) else f"ç±»åˆ«{class_id}"
            print(f"  {class_name} (ç±»åˆ«{class_id}): {count} ä¸ªå®ä¾‹ ({percentage:.1f}%)")
        
        self.class_distribution = dict(class_counts)
        return self.class_distribution
    
    def calculate_class_weights(self) -> Dict[int, float]:
        """
        è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆç”¨äºå¹³è¡¡æŸå¤±å‡½æ•°ï¼‰
        
        Returns:
            ç±»åˆ«æƒé‡å­—å…¸ {class_id: weight}
        """
        if self.class_distribution is None:
            self.analyze_class_distribution()
        
        num_classes = self.data_config.get('nc', 0)
        total_instances = sum(self.class_distribution.values())
        
        class_weights = {}
        
        print("\nç±»åˆ«æƒé‡è®¡ç®—:")
        for class_id in range(num_classes):
            count = self.class_distribution.get(class_id, 0)
            
            # è®¡ç®—æƒé‡ï¼ˆå®ä¾‹æ•°è¶Šå°‘ï¼Œæƒé‡è¶Šé«˜ï¼‰
            if count > 0:
                class_weights[class_id] = total_instances / (num_classes * count)
            else:
                class_weights[class_id] = 1.0
            
            class_name = self.data_config['names'][class_id] if class_id < len(self.data_config['names']) else f"ç±»åˆ«{class_id}"
            print(f"  {class_name}: {class_weights[class_id]:.3f}")
        
        return class_weights
    
    def analyze_image_sizes(self) -> Dict[str, any]:
        """
        åˆ†æå›¾ç‰‡å°ºå¯¸åˆ†å¸ƒ
        
        Returns:
            å›¾ç‰‡å°ºå¯¸ç»Ÿè®¡ä¿¡æ¯
        """
        print("ğŸ“ åˆ†æå›¾ç‰‡å°ºå¯¸åˆ†å¸ƒ...")
        
        image_sizes = []
        
        # åˆ†æè®­ç»ƒé›†å›¾ç‰‡
        train_img_dir = self.base_path / 'images' / 'train'
        if train_img_dir.exists():
            for img_file in train_img_dir.glob("*.jpg"):
                try:
                    import cv2
                    img = cv2.imread(str(img_file))
                    if img is not None:
                        h, w = img.shape[:2]
                        image_sizes.append((w, h))
                except:
                    continue
        
        # åˆ†æéªŒè¯é›†å›¾ç‰‡
        val_img_dir = self.base_path / 'images' / 'val'
        if val_img_dir.exists():
            for img_file in val_img_dir.glob("*.jpg"):
                try:
                    import cv2
                    img = cv2.imread(str(img_file))
                    if img is not None:
                        h, w = img.shape[:2]
                        image_sizes.append((w, h))
                except:
                    continue
        
        if image_sizes:
            widths, heights = zip(*image_sizes)
            
            stats = {
                'total_images': len(image_sizes),
                'avg_width': np.mean(widths),
                'avg_height': np.mean(heights),
                'min_width': min(widths),
                'max_width': max(widths),
                'min_height': min(heights),
                'max_height': max(heights),
                'size_distribution': image_sizes
            }
            
            print(f"å›¾ç‰‡å°ºå¯¸ç»Ÿè®¡:")
            print(f"  æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
            print(f"  å¹³å‡å°ºå¯¸: {stats['avg_width']:.0f} x {stats['avg_height']:.0f}")
            print(f"  å°ºå¯¸èŒƒå›´: {stats['min_width']}x{stats['min_height']} - {stats['max_width']}x{stats['max_height']}")
            
            return stats
        else:
            print("âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶")
            return {'total_images': 0}
    
    def generate_analysis_report(self) -> str:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Returns:
            åˆ†ææŠ¥å‘Šå­—ç¬¦ä¸²
        """
        print("ğŸ“ ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š...")
        
        # åŸºç¡€ç»Ÿè®¡
        basic_stats = get_dataset_stats(self.data_yaml_path)
        
        # ç±»åˆ«åˆ†å¸ƒ
        if self.class_distribution is None:
            self.analyze_class_distribution()
        
        # ç±»åˆ«æƒé‡
        class_weights = self.calculate_class_weights()
        
        # å›¾ç‰‡å°ºå¯¸
        image_stats = self.analyze_image_sizes()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("=" * 60)
        report.append("é“è·¯ç—…å®³æ£€æµ‹æ•°æ®é›†åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")
        
        # åŸºç¡€ç»Ÿè®¡
        report.append("ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        report.append(f"  è®­ç»ƒå›¾ç‰‡: {basic_stats['train_count']} å¼ ")
        report.append(f"  éªŒè¯å›¾ç‰‡: {basic_stats['val_count']} å¼ ")
        report.append(f"  ç±»åˆ«æ•°é‡: {basic_stats['num_classes']}")
        report.append(f"  ç±»åˆ«åç§°: {', '.join(basic_stats['class_names'])}")
        report.append("")
        
        # ç±»åˆ«åˆ†å¸ƒ
        report.append("ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
        total_instances = sum(self.class_distribution.values())
        for class_id, count in self.class_distribution.items():
            percentage = (count / total_instances * 100) if total_instances > 0 else 0
            class_name = self.data_config['names'][class_id] if class_id < len(self.data_config['names']) else f"ç±»åˆ«{class_id}"
            report.append(f"  {class_name}: {count} ä¸ªå®ä¾‹ ({percentage:.1f}%)")
        report.append("")
        
        # ç±»åˆ«æƒé‡
        report.append("âš–ï¸ ç±»åˆ«æƒé‡:")
        for class_id, weight in class_weights.items():
            class_name = self.data_config['names'][class_id] if class_id < len(self.data_config['names']) else f"ç±»åˆ«{class_id}"
            report.append(f"  {class_name}: {weight:.3f}")
        report.append("")
        
        # å›¾ç‰‡å°ºå¯¸
        if image_stats['total_images'] > 0:
            report.append("ğŸ“ å›¾ç‰‡å°ºå¯¸:")
            report.append(f"  æ€»å›¾ç‰‡æ•°: {image_stats['total_images']}")
            report.append(f"  å¹³å‡å°ºå¯¸: {image_stats['avg_width']:.0f} x {image_stats['avg_height']:.0f}")
            report.append(f"  å°ºå¯¸èŒƒå›´: {image_stats['min_width']}x{image_stats['min_height']} - {image_stats['max_width']}x{image_stats['max_height']}")
            report.append("")
        
        # è®­ç»ƒå»ºè®®
        report.append("ğŸ’¡ è®­ç»ƒå»ºè®®:")
        max_count = max(self.class_distribution.values())
        min_count = min(self.class_distribution.values())
        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
        
        if imbalance_ratio > 3:
            report.append(f"  âš ï¸  æ£€æµ‹åˆ°ç±»åˆ«ä¸å¹³è¡¡ (æ¯”ä¾‹: {imbalance_ratio:.1f}:1)")
            report.append("     å»ºè®®: ä½¿ç”¨ç±»åˆ«åŠ æƒæŸå¤±å‡½æ•°æˆ–è¿‡é‡‡æ ·ç­–ç•¥")
        else:
            report.append("  âœ… ç±»åˆ«åˆ†å¸ƒç›¸å¯¹å‡è¡¡")
        
        if image_stats['total_images'] > 0:
            avg_size = (image_stats['avg_width'] + image_stats['avg_height']) / 2
            if avg_size > 1000:
                report.append("  ğŸ“ å›¾ç‰‡å°ºå¯¸è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå¤§çš„è¾“å…¥å°ºå¯¸")
            elif avg_size < 500:
                report.append("  ğŸ“ å›¾ç‰‡å°ºå¯¸è¾ƒå°ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå°çš„è¾“å…¥å°ºå¯¸")
        
        report.append("")
        report.append("=" * 60)
        
        return "\n".join(report)