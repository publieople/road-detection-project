#!/usr/bin/env python3
"""
é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºyolo11è®­ç»ƒä¸“é—¨çš„é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹
"""

import torch
from ultralytics import YOLO # pyright: ignore[reportPrivateImportUsage]
from pathlib import Path
import yaml
import os

def get_dataset_stats(data_yaml_path: str) -> dict:
    """ä»æ•°æ®é…ç½®æ–‡ä»¶ä¸­è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        print(f"ğŸ“Š æ­£åœ¨åˆ†ææ•°æ®é›†é…ç½®: {data_yaml_path}")

        with open(data_yaml_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)

        # è·å–ç±»åˆ«ä¿¡æ¯
        nc = data_config.get('nc', 0)
        names = data_config.get('names', [])

        print(f"ğŸ¯ ç±»åˆ«æ•°é‡: {nc}")
        print(f"ğŸ·ï¸  ç±»åˆ«åç§°: {names}")

        # è·å–åŸºç¡€è·¯å¾„ - ç›´æ¥ä½¿ç”¨YAMLæ–‡ä»¶æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºç¡€è·¯å¾„
        yaml_dir = Path(data_yaml_path).parent
        base_path = yaml_dir

        print(f"ğŸ“‚ YAMLæ–‡ä»¶æ‰€åœ¨ç›®å½•: {base_path}")

        # è®¡ç®—è®­ç»ƒå’ŒéªŒè¯å›¾ç‰‡æ•°é‡
        def count_images_and_labels(train_val_path):
            """è®¡ç®—æŒ‡å®šè·¯å¾„ä¸‹çš„å›¾ç‰‡å’Œæ ‡ç­¾æ•°é‡"""
            if not train_val_path:
                return 0, 0

            # æ„å»ºå®Œæ•´çš„å›¾ç‰‡è·¯å¾„
            image_path = base_path / train_val_path

            print(f"\nğŸ” æ£€æŸ¥è·¯å¾„: {image_path}")

            if not image_path.exists():
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {image_path}")
                return 0, 0

            # ç»Ÿè®¡å›¾ç‰‡æ–‡ä»¶
            image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']
            image_files = []

            # è·å–æ‰€æœ‰æ–‡ä»¶ï¼Œç„¶åæŒ‰æ‰©å±•åè¿‡æ»¤
            for file_path in image_path.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    image_files.append(file_path)

            # å»é‡ï¼ˆæŒ‰æ–‡ä»¶åï¼‰
            unique_files = list(set(image_files))
            total_images = len(unique_files)
            print(f"ğŸ“¸ æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {total_images} å¼ ")

            # æ£€æŸ¥å¯¹åº”çš„æ ‡ç­¾è·¯å¾„
            label_path = Path(str(image_path).replace('images', 'labels'))
            print(f"ğŸ·ï¸  æ ‡ç­¾è·¯å¾„: {label_path}")

            if label_path.exists():
                # ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶
                label_files = list(label_path.rglob('*.txt'))
                total_labels = len(label_files)
                print(f"ğŸ“ æ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {total_labels} ä¸ª")

                # æ£€æŸ¥åŒ¹é…æƒ…å†µ
                if total_images > 0:
                    match_ratio = (total_labels / total_images) * 100
                    print(f"âœ… å›¾ç‰‡-æ ‡ç­¾åŒ¹é…ç‡: {match_ratio:.1f}%")

                    if match_ratio < 100:
                        print(f"âš ï¸  è­¦å‘Š: {total_images - total_labels} å¼ å›¾ç‰‡ç¼ºå°‘æ ‡ç­¾æ–‡ä»¶")

                        # åˆ—å‡ºå‰10ä¸ªæ²¡æœ‰æ ‡ç­¾çš„å›¾ç‰‡
                        missing_labels = []
                        for img_file in image_files[:10]:  # åªæ£€æŸ¥å‰10ä¸ª
                            expected_label = label_path / (img_file.stem + '.txt')
                            if not expected_label.exists():
                                missing_labels.append(img_file.name)

                        if missing_labels:
                            print(f"   ç¼ºå¤±æ ‡ç­¾çš„å›¾ç‰‡ç¤ºä¾‹: {missing_labels[:5]}")
            else:
                print(f"âŒ æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {label_path}")
                total_labels = 0

            return total_images, total_labels

        # ç»Ÿè®¡è®­ç»ƒé›†
        train_path = data_config.get('train', 'images/train')
        train_images, train_labels = count_images_and_labels(train_path)

        # ç»Ÿè®¡éªŒè¯é›†
        val_path = data_config.get('val', 'images/val')
        val_images, val_labels = count_images_and_labels(val_path)

        # æ€»è®¡
        total_images = train_images + val_images
        total_labels = train_labels + val_labels

        print(f"\næ•°æ®é›†ç»Ÿè®¡æ€»ç»“:")
        print("=" * 60)
        print(f"è®­ç»ƒé›†: {train_images} å¼ å›¾ç‰‡, {train_labels} ä¸ªæ ‡ç­¾")
        print(f"éªŒè¯é›†: {val_images} å¼ å›¾ç‰‡, {val_labels} ä¸ªæ ‡ç­¾")
        print(f"æ€»è®¡: {total_images} å¼ å›¾ç‰‡, {total_labels} ä¸ªæ ‡ç­¾")

        # YOLOè®­ç»ƒæ—¶çš„å®é™…ä½¿ç”¨æ•°é‡ï¼ˆæœ‰æ ‡ç­¾çš„å›¾ç‰‡ï¼‰
        usable_train = min(train_images, train_labels)
        usable_val = min(val_images, val_labels)
        usable_total = usable_train + usable_val

        print(f"\nYOLOè®­ç»ƒå®é™…å¯ç”¨:")
        print(f"   è®­ç»ƒé›†: {usable_train} å¼ å›¾ç‰‡")
        print(f"   éªŒè¯é›†: {usable_val} å¼ å›¾ç‰‡")
        print(f"   æ€»è®¡: {usable_total} å¼ å›¾ç‰‡")

        if usable_total < total_images:
            print(f"è­¦å‘Š: ç”±äºç¼ºå°‘æ ‡ç­¾æ–‡ä»¶ï¼ŒYOLOå°†åªä½¿ç”¨ {usable_total}/{total_images} å¼ å›¾ç‰‡")

        return {
            'train_count': train_labels,  # å®é™…æœ‰æ ‡ç­¾çš„è®­ç»ƒå›¾ç‰‡æ•°é‡
            'val_count': val_labels,      # å®é™…æœ‰æ ‡ç­¾çš„éªŒè¯å›¾ç‰‡æ•°é‡
            'total_images': total_images, # æ€»å›¾ç‰‡æ•°é‡
            'total_labels': total_labels, # æ€»æ ‡ç­¾æ•°é‡
            'num_classes': nc,
            'class_names': names
        }

    except Exception as e:
        print(f"âš ï¸  è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'train_count': 0,
            'val_count': 0,
            'total_images': 0,
            'total_labels': 0,
            'num_classes': 0,
            'class_names': []
        }

def setup_training():
    """é…ç½®è®­ç»ƒç¯å¢ƒå’Œå‚æ•°"""

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ”¥ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        device = 'cuda'
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
        device = 'cpu'

    return device

def train_model(data_yaml_path: str, model_size: str = 'n', epochs: int = 100, img_size: int = 1280, resume: bool = False):
    """
    è®­ç»ƒYOLOæ¨¡å‹

    Args:
        data_yaml_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        model_size: æ¨¡å‹å¤§å° ('n', 's', 'm', 'l', 'x')
        epochs: è®­ç»ƒè½®æ•°
        img_size: è¾“å…¥å›¾åƒå°ºå¯¸
        resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤è®­ç»ƒ
    """

    device = setup_training()

    if resume:
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        print("ğŸ”„ æ£€æµ‹æ˜¯å¦å­˜åœ¨ä¸­æ–­çš„è®­ç»ƒï¼Œå°è¯•æ¢å¤...")

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸Šæ¬¡è®­ç»ƒçš„æƒé‡æ–‡ä»¶
        possible_weights = [
            'runs/detect/train/weights/last.pt',  # é»˜è®¤è®­ç»ƒè·¯å¾„
            'runs/detect/train2/weights/last.pt', # ç¬¬äºŒæ¬¡è®­ç»ƒè·¯å¾„
            'runs/detect/train3/weights/last.pt', # ç¬¬ä¸‰æ¬¡è®­ç»ƒè·¯å¾„
        ]

        resume_path = None
        for weight_path in possible_weights:
            if Path(weight_path).exists():
                resume_path = weight_path
                break

        if resume_path:
            print(f"âœ… æ‰¾åˆ°ä¸­æ–­çš„è®­ç»ƒæƒé‡: {resume_path}")
            model = YOLO(resume_path)
            print("ğŸš€ ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­è®­ç»ƒ...")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°å¯æ¢å¤çš„æƒé‡æ–‡ä»¶ï¼Œå¼€å§‹æ–°çš„è®­ç»ƒ...")
            # é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹
            model_name = f'yolo11{model_size}.pt'
            print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
            model = YOLO(model_name)
    else:
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼
        # é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹
        model_name = f'yolo11{model_size}.pt'
        print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
        model = YOLO(model_name)

    # è®­ç»ƒé…ç½®
    training_config = {
        'data': data_yaml_path,
        'epochs': epochs,
        'imgsz': img_size,
        'batch': 16,                             # æ‰¹æ¬¡å¤§å°ï¼Œå¯æ ¹æ®GPUå†…å­˜è°ƒæ•´
        'workers': 4,                            # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
        'cache': False,                          # æ˜¯å¦ç¼“å­˜æ•°æ®é›†

        'device': device,
        'optimizer': 'SGD',
        'lr0': 0.001,                            # åˆå§‹å­¦ä¹ ç‡
        'lrf': 0.1,                              # æœ€ç»ˆå­¦ä¹ ç‡å€æ•°
        'momentum': 0.9,
        'weight_decay': 0.0005,
        'warmup_epochs': 5,                      # é¢„çƒ­è½®æ•°
        'warmup_momentum': 0.8,
        'box': 7.5,                              # box losså¢ç›Š
        'cls': 0.5,                              # cls losså¢ç›Š
        'dfl': 1.5,                              # dfl losså¢ç›Š

        # é¢œè‰²å¢å¼º
        'hsv_h': 0.015,                          # HSVè‰²è°ƒå¢å¼º
        'hsv_s': 0.7,                            # HSVé¥±å’Œåº¦å¢å¼º
        'hsv_v': 0.4,                            # HSVæ˜åº¦å¢å¼º

        # å‡ ä½•å¢å¼º
        'degrees': 10.0,                         # æ—‹è½¬å¢å¼º
        'scale': 0.7,                            # ç¼©æ”¾å¢å¼º
        'shear': 0.0,                            # å‰ªåˆ‡å¢å¼º
        'translate': 0.2,                        # å¹³ç§»å¢å¼º
        'perspective': 0.0005,                   # é€è§†å¢å¼º
        'fliplr': 0.8,                           # å·¦å³ç¿»è½¬
        'flipud': 0.3,                           # ä¸Šä¸‹ç¿»è½¬

        'mosaic': 0.5,                           # é™ä½mosaicå¢å¼ºå¼ºåº¦
        'mixup': 0.3,                            # å‡å°‘mixupå¢å¼ºæ¯”ä¾‹
        'copy_paste': 0.2,                       # é™ä½å¤åˆ¶ç²˜è´´å¢å¼ºæ¯”ä¾‹
        'auto_augment': 'rand-m9-mstd0.5-inc1',  # è‡ªåŠ¨å¢å¼ºç­–ç•¥
        'erasing': 0.6,                          # éšæœºæ“¦é™¤

        'crop_fraction': 1.0,                    # è£å‰ªæ¯”ä¾‹
        "amp": True,                             # æ··åˆç²¾åº¦åŠ é€Ÿ

        'close_mosaic': 10,                      # å…³é—­mosaicçš„æœ€åè½®æ•°
        'overlap_mask': False,                   # æ˜¯å¦ä½¿ç”¨é‡å æ©ç 
        'single_cls': False,                     # æ˜¯å¦ä¸ºå•ç±»åˆ«æ£€æµ‹
        'patience': 50,                          # æ—©åœè€å¿ƒå€¼
        'cos_lr': True,                          # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    }

    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    print(f"ğŸ“Š è®­ç»ƒè½®æ•°: {epochs}")
    print(f"ğŸ“ å›¾åƒå°ºå¯¸: {img_size}")
    print(f"ğŸ”§ è®¾å¤‡: {device}")

    # å¼€å§‹è®­ç»ƒ
    if resume and resume_path:
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        training_config['resume'] = True
        results = model.train(**training_config)
    else:
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼
        results = model.train(**training_config)

    # ä¿å­˜è®­ç»ƒç»“æœ
    print("âœ… è®­ç»ƒå®Œæˆ!")
    if results and hasattr(results, 'save_dir'):
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {Path(results.save_dir).resolve()}")
    else:
        print("ğŸ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è·¯å¾„ä¿¡æ¯ä¸å¯ç”¨")

    return model, results

def validate_model(model, data_yaml_path: str):
    """éªŒè¯æ¨¡å‹æ€§èƒ½"""
    print("ğŸ” éªŒè¯æ¨¡å‹æ€§èƒ½...")

    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    metrics = model.val(data=data_yaml_path)

    print("ğŸ“Š éªŒè¯ç»“æœ:")
    print(f"   mAP@0.5: {metrics.box.map50:.3f}")
    print(f"   mAP@0.5:0.95: {metrics.box.map:.3f}")
    print(f"   å¹³å‡ç²¾ç¡®ç‡: {metrics.box.mp:.3f}")
    print(f"   å¹³å‡å¬å›ç‡: {metrics.box.mr:.3f}")

    return metrics

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--resume', action='store_true', help='ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤è®­ç»ƒ')
    parser.add_argument('--data', type=str, default='datasets/yolo_format/road.yaml', help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model-size', type=str, default='n', choices=['n', 's', 'm', 'l', 'x'], help='æ¨¡å‹å¤§å°')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--img-size', type=int, default=640, help='è¾“å…¥å›¾åƒå°ºå¯¸')

    args = parser.parse_args()

    # æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
    data_yaml = args.data

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(data_yaml).exists():
        print(f"âŒ æ•°æ®é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_yaml}")
        return

    print("ğŸ›£ï¸  é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)

    if args.resume:
        print("ğŸ”„ å·²å¯ç”¨è®­ç»ƒæ¢å¤æ¨¡å¼")

    print(f"ğŸ“Š é…ç½®: æ¨¡å‹={args.model_size}, è½®æ•°={args.epochs}, å°ºå¯¸={args.img_size}")

    try:
        # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        dataset_stats = get_dataset_stats(data_yaml)

        # è®­ç»ƒæ¨¡å‹
        model, training_results = train_model(
            data_yaml_path=data_yaml,
            model_size=args.model_size,
            epochs=args.epochs,
            img_size=args.img_size,
            resume=args.resume
        )

        # éªŒè¯æ¨¡å‹
        metrics = validate_model(model, data_yaml)


        print("\nğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
        print("ğŸ“‹ æ€»ç»“:")
        print(f"   - è®­ç»ƒå›¾ç‰‡: {dataset_stats['train_count']}å¼ ")
        print(f"   - éªŒè¯å›¾ç‰‡: {dataset_stats['val_count']}å¼ ")
        print(f"   - ç—…å®³ç±»åˆ«: {dataset_stats['num_classes']}ç±»")
        if dataset_stats['class_names']:
            print(f"   - ç±»åˆ«åç§°: {', '.join(dataset_stats['class_names'])}")
        print(f"   - æœ€ä½³mAP@0.5: {metrics.box.map50:.3f}")

        # å¯¼å‡ºæ¨¡å‹
        print("\nğŸ’¾ å¯¼å‡ºè®­ç»ƒå¥½çš„æ¨¡å‹...")
        model.export(format='onnx', simplify=True)

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()