#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹4ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å’Œé«˜å‡†ç¡®ç‡éœ€æ±‚
"""

import torch
from ultralytics import YOLO # pyright: ignore[reportPrivateImportUsage]
from pathlib import Path
import yaml
import os
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from typing import Optional

# é…ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ
try:
    # Windowsç³»ç»Ÿå¸¸è§ä¸­æ–‡å­—ä½“
    font_paths = [
        "C:/Windows/Fonts/simhei.ttf",  # é»‘ä½“
        "C:/Windows/Fonts/simsun.ttc",  # å®‹ä½“
        "C:/Windows/Fonts/msyh.ttc",    # å¾®è½¯é›…é»‘
    ]

    # æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    available_fonts = []
    for font_path in font_paths:
        if Path(font_path).exists():
            available_fonts.append(font_path)

    if available_fonts:
        primary_font = available_fonts[0]
        font_name = Path(primary_font).stem
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['font.sans-serif'] = [font_name, 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        fm.fontManager.addfont(primary_font)

except Exception as e:
    print("ä¸­æ–‡å­—ä½“é…ç½®å¤±è´¥: {}".format(e))

def analyze_dataset_labels(data_yaml_path: str) -> dict:
    """
    åˆ†ææ•°æ®é›†ä¸­çš„æ ‡ç­¾åˆ†å¸ƒï¼Œè®¡ç®—ç±»åˆ«æƒé‡
    """
    print(f"ğŸ“Š åˆ†ææ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ: {data_yaml_path}")

    with open(data_yaml_path, 'r', encoding='utf-8') as f:
        data_config = yaml.safe_load(f)

    base_path = Path(data_yaml_path).parent
    train_path = base_path / data_config['train']
    label_path = base_path / 'labels' / 'train'

    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å®ä¾‹æ•°é‡
    class_counts = Counter()
    total_instances = 0

    if label_path.exists():
        for txt_file in label_path.glob("*.txt"):
            with open(txt_file, 'r') as f:
                for line in f:
                    if line.strip():
                        class_id = int(line.strip().split()[0])
                        class_counts[class_id] += 1
                        total_instances += 1

    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆç”¨äºå¹³è¡¡æŸå¤±å‡½æ•°ï¼‰
    num_classes = data_config.get('nc', 4)
    class_weights = {}

    print("ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
    for class_id in range(num_classes):
        count = class_counts.get(class_id, 0)
        percentage = (count / total_instances * 100) if total_instances > 0 else 0
        class_name = data_config['names'][class_id]
        print(f"  {class_name} (ç±»åˆ«{class_id}): {count} ä¸ªå®ä¾‹ ({percentage:.1f}%)")

        # è®¡ç®—æƒé‡ï¼ˆå®ä¾‹æ•°è¶Šå°‘ï¼Œæƒé‡è¶Šé«˜ï¼‰
        if count > 0:
            class_weights[class_id] = total_instances / (num_classes * count)
        else:
            class_weights[class_id] = 1.0

    print(f"\nç±»åˆ«æƒé‡ (ç”¨äºæŸå¤±å‡½æ•°å¹³è¡¡):")
    for class_id, weight in class_weights.items():
        class_name = data_config['names'][class_id]
        print(f"  {class_name}: {weight:.3f}")

    return {
        'class_counts': dict(class_counts),
        'class_weights': class_weights,
        'total_instances': total_instances
    }

def setup_training():
    """é…ç½®è®­ç»ƒç¯å¢ƒå’Œå‚æ•°"""

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if torch.cuda.is_available():
        print("GPUå¯ç”¨: {}".format(torch.cuda.get_device_name(0)))
        print("CUDAç‰ˆæœ¬: {}".format(torch.version.cuda))
        device = 'cuda'
    else:
        print("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
        device = 'cpu'

    return device

def train_optimized_model(data_yaml_path: str, model_size: str = 's', epochs: int = 150,
                         img_size: int = 640, resume_path: Optional[str] = None):
    """
    è®­ç»ƒYOLOæ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹é“è·¯ç—…å®³æ£€æµ‹

    Args:
        data_yaml_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        model_size: æ¨¡å‹å¤§å° ('n', 's', 'm', 'l', 'x')
        epochs: è®­ç»ƒè½®æ•°
        img_size: è¾“å…¥å›¾åƒå°ºå¯¸
        resume_path: æ¢å¤è®­ç»ƒçš„æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™å¼€å§‹æ–°è®­ç»ƒ
    """
    device = setup_training()

    # åˆ†ææ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ
    dataset_stats = analyze_dataset_labels(data_yaml_path)
    class_weights = dataset_stats['class_weights']

    if resume_path:
        if not Path(resume_path).exists():
            print(f"âŒ æŒ‡å®šçš„æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {resume_path}")
            print("âš ï¸  å¼€å§‹æ–°çš„è®­ç»ƒ...")
            model_name = f'yolo11{model_size}.pt'
            print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
            model = YOLO(model_name)
        else:
            print(f"ğŸ”„ ä»æŒ‡å®šæƒé‡æ¢å¤è®­ç»ƒ: {resume_path}")
            model = YOLO(resume_path)
            print("ğŸš€ ç»§ç»­è®­ç»ƒ...")
    else:
        model_name = f'yolo11{model_size}.pt'
        print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
        model = YOLO(model_name)

    # è®­ç»ƒé…ç½®
    training_config = {
        'data': data_yaml_path,
        'epochs': epochs,
        'imgsz': img_size,
        'batch': 16,  # æ‰¹æ¬¡å¤§å°
        'workers': 4,  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
        'cache': False, # Windows ä¸‹éœ€ç¦ç”¨
        'device': device,

        # ä¼˜åŒ–å™¨é…ç½®
        'optimizer': 'AdamW',  # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
        'lr0': 0.002,  # åˆå§‹å­¦ä¹ ç‡
        'lrf': 0.05,  # æœ€ç»ˆå­¦ä¹ ç‡å€æ•°
        'momentum': 0.95,
        'weight_decay': 0.0001, # æƒé‡è¡°å‡

        # å­¦ä¹ ç‡è°ƒåº¦
        'warmup_epochs': 3,  # é¢„çƒ­è½®æ•°
        'warmup_momentum': 0.8,
        'cos_lr': True,  # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡

        # æŸå¤±å‡½æ•°é…ç½®ï¼ˆé’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡ï¼‰
        'box': 9.0,  # box losså¢ç›Šï¼Œæå‡å®šä½ç²¾åº¦
        'cls': 3.0,  # cls losså¢ç›Šï¼Œå¢å¼ºåˆ†ç±»æ•ˆæœ
        'dfl': 2.5,  # dfl losså¢ç›Š

        # æ•°æ®å¢å¼º
        'hsv_h': 0.015,  # HSVè‰²è°ƒå¢å¼º
        'hsv_s': 0.7,  # HSVé¥±å’Œåº¦å¢å¼º
        'hsv_v': 0.4,  # HSVæ˜åº¦å¢å¼º

        # å‡ ä½•å¢å¼º
        'degrees': 15.0,  # æ—‹è½¬å¢å¼º
        'translate': 0.4,  # å¹³ç§»å¢å¼º
        'scale': 0.9,  # ç¼©æ”¾å¢å¼º
        'shear': 5.0,  # å‰ªåˆ‡å¢å¼º
        'perspective': 0.001,  # é€è§†å¢å¼º
        'fliplr': 0.8,  # å·¦å³ç¿»è½¬
        'flipud': 0.2,  # ä¸Šä¸‹ç¿»è½¬

        # é«˜çº§å¢å¼º
        'mosaic': 0.8,  # Mosaicå¢å¼º
        'mixup': 0.3,  # MixUpå¢å¼º
        'copy_paste': 0.4,  # å¤åˆ¶ç²˜è´´å¢å¼º
        'auto_augment': 'rand-m9-mstd0.5-inc1',  # è‡ªåŠ¨å¢å¼º
        'erasing': 0.4,  # éšæœºæ“¦é™¤

        # è®­ç»ƒç­–ç•¥
        'close_mosaic': 20,  # åæœŸå…³é—­Mosaic
        'patience': 15,  # æ—©åœè€å¿ƒå€¼
        'single_cls': False,  # å¤šç±»åˆ«æ£€æµ‹

        # æ€§èƒ½ä¼˜åŒ–
        'amp': True,  # æ··åˆç²¾åº¦è®­ç»ƒ
        'compile': False,  # æ¨¡å‹ç¼–è¯‘ï¼ˆå¯é€‰ï¼‰

        # éªŒè¯å’Œè¯„ä¼°
        'val': True,
        'split': 'val',
        'save': True,
        'save_period': 10,  # æ¯10è½®ä¿å­˜ä¸€æ¬¡
        'plots': True,  # ç”Ÿæˆå›¾è¡¨

        # ä¿å­˜è·¯å¾„é…ç½® - ç¡®ä¿ä¿å­˜åˆ°å½“å‰ç›®å½•
        'project': './runs',                     # é¡¹ç›®ç›®å½•
        'name': 'detect',                        # å®éªŒåç§°
    }

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒè½®æ•°: {epochs}")
    print(f"ğŸ“ å›¾åƒå°ºå¯¸: {img_size}")
    print(f"ğŸ”§ è®¾å¤‡: {device}")
    print(f"ğŸ“¦ æ¨¡å‹å¤§å°: {model_size}")

    # å¼€å§‹è®­ç»ƒ
    if resume_path and Path(resume_path).exists():
        training_config['resume'] = True
        results = model.train(**training_config)
    else:
        results = model.train(**training_config)

    print("âœ… è®­ç»ƒå®Œæˆ!")
    return model, results, dataset_stats

def validate_model(model, data_yaml_path: str):
    """éªŒè¯æ¨¡å‹æ€§èƒ½"""
    print("ğŸ” éªŒè¯æ¨¡å‹æ€§èƒ½...")

    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    metrics = model.val(data=data_yaml_path)

    print("ğŸ“Š éªŒè¯ç»“æœ:")
    print(f"   mAP@0.5: {metrics.box.map50:.3f}")
    print(f"   mAP@0.5:0.95: {metrics.box.map:.3f}")
    print(f"   å¹³å‡ç²¾ç¡®ç‡: {metrics.box.mp:.3f}")
    print(f"   å¹³å‡å¬å›ç‡: {metrics.box.mr:.3f}")

    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
    if hasattr(metrics.box, 'ap50'):
        print("\nå„ç±»åˆ«AP@0.5:")
        with open(data_yaml_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)
        class_names = data_config['names']

        for i, ap50 in enumerate(metrics.box.ap50):
            if i < len(class_names):
                print(f"   {class_names[i]}: {ap50:.3f}")

    return metrics

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--resume', type=str, help='ä»æŒ‡å®šæƒé‡æ–‡ä»¶æ¢å¤è®­ç»ƒè·¯å¾„')
    parser.add_argument('--data', type=str, default='datasets/yolo_format/road.yaml', help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model-size', type=str, default='s', choices=['n', 's', 'm', 'l', 'x'], help='æ¨¡å‹å¤§å°')
    parser.add_argument('--epochs', type=int, default=150, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--img-size', type=int, default=640, help='è¾“å…¥å›¾åƒå°ºå¯¸')

    args = parser.parse_args()

    # æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
    data_yaml = args.data

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(data_yaml).exists():
        print(f"âŒ æ•°æ®é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_yaml}")
        return

    print("ğŸ›£ï¸  é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    if args.resume:
        print(f"ğŸ”„ å·²å¯ç”¨è®­ç»ƒæ¢å¤æ¨¡å¼ï¼Œæƒé‡æ–‡ä»¶: {args.resume}")

    print(f"ğŸ“Š é…ç½®: æ¨¡å‹={args.model_size}, è½®æ•°={args.epochs}, å°ºå¯¸={args.img_size}")

    try:
        # è®­ç»ƒæ¨¡å‹
        model, training_results, dataset_stats = train_optimized_model(
            data_yaml_path=data_yaml,
            model_size=args.model_size,
            epochs=args.epochs,
            img_size=args.img_size,
            resume_path=args.resume
        )

        # éªŒè¯æ¨¡å‹
        metrics = validate_model(model, data_yaml)

        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("ğŸ“‹ è®­ç»ƒæ€»ç»“:")
        print(f"   - è®­ç»ƒå›¾ç‰‡: {dataset_stats['total_instances']} ä¸ªå®ä¾‹")
        print(f"   - ç—…å®³ç±»åˆ«: 4ç±»")
        print(f"   - ç±»åˆ«åˆ†å¸ƒ: {dataset_stats['class_counts']}")
        print(f"   - æœ€ä½³mAP@0.5: {metrics.box.map50:.3f}")

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡
        if metrics.box.map50 >= 0.80:
            print("ğŸ¯ ç›®æ ‡è¾¾æˆï¼æ¨¡å‹å‡†ç¡®ç‡ â‰¥ 80%")
        else:
            print(f"âš ï¸  æœªè¾¾ç›®æ ‡ã€‚å½“å‰å‡†ç¡®ç‡: {metrics.box.map50:.1%}, ç›®æ ‡: 80%")
            print("ğŸ’¡ å»ºè®®: å¢åŠ è®­ç»ƒè½®æ•°ã€è°ƒæ•´è¶…å‚æ•°æˆ–æ”¶é›†æ›´å¤šæ•°æ®")

        # å¯¼å‡ºæ¨¡å‹
        print("\nğŸ’¾ å¯¼å‡ºè®­ç»ƒå¥½çš„æ¨¡å‹...")
        model.export(format='onnx', simplify=True)

        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        try:
            save_dir = getattr(training_results, 'save_dir', None)
            if save_dir:
                report_path = Path(save_dir) / "training_report.txt"
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write("é“è·¯ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒæŠ¥å‘Š\n")
                    f.write("=" * 50 + "\n")
                    training_time = getattr(training_results, 'time', 'æœªçŸ¥')
                    f.write(f"è®­ç»ƒæ—¶é—´: {training_time}\n")
                    f.write(f"æœ€ä½³mAP@0.5: {metrics.box.map50:.3f}\n")
                    f.write(f"æœ€ç»ˆmAP@0.5:0.95: {metrics.box.map:.3f}\n")
                    f.write(f"ç±»åˆ«åˆ†å¸ƒ: {dataset_stats['class_counts']}\n")
                    f.write(f"ç±»åˆ«æƒé‡: {dataset_stats['class_weights']}\n")
                    f.write("=" * 50 + "\n")
                    f.write("æ¨¡å‹é…ç½®å’Œè¶…å‚æ•°è¯¦è§ args.yaml æ–‡ä»¶\n")

                print(f"ğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            else:
                print("âš ï¸  æ— æ³•ä¿å­˜è®­ç»ƒæŠ¥å‘Šï¼Œç¼ºå°‘ä¿å­˜è·¯å¾„ä¿¡æ¯")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è®­ç»ƒæŠ¥å‘Šæ—¶å‡ºé”™: {e}")

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()